Bu projede amaç, prosedürel olarak üretilen dikdörtgen engellerle dolu 2B haritalarda öğrenen bir politika ile klasik bir planlayıcıyı aynı koşullarda karşılaştırmakdı. Ortam üstten görünüş (top-down) olarak Pygame ile çiziliyor; arena 768×576 piksel ve robotumuz dairesel, yarıçapı 12 piksel. Engeller eksenlere paralel dikdörtgenler; konum ve boyutlar rejection sampling ile üretiliyor ve zorluk seviyeleri engel sayısı ile kontrol ediliyor. Başlangıç genelde sol, hedef sağ tarafa yakın olacak şekilde seçiliyor fakat her bölümün tekrarlanabilir olması için seed kullanılıyor.

Ajanın gözlem alanı 31 elemanlı bir vektör. Bu vektörde kendi pozisyonumuzun normalleştirilmiş hali, heading olarak baş yönümüzün birim vektörü, hedefe olan normalleştirilmiş mesafe ve yön bilgisi var; ayrıca 360 derece etrafı tarayan 24 adet ray-cast sensöründen gelen yakınlık okumaları bulunuyor. Bu 24 ray, ajan merkezinden gönderilip ilk çarpışma mesafesini ölçüyor ve genelde sensör menziline bölünerek [0,1] aralığında normalleştiriliyor. Gözlem vektöründeki bu düzen sayesinde politika farklı haritalarda genelleme yapabiliyor.

Aksiyon uzayı üç ayrık eylemden oluşuyor: ileri gitme, sağa dönüş, sola dönüş. İleri eylemi belirli bir adım öne gitmeyi sağlıyor; dönüşler yerinde veya küçük açılarla yapılıyor. İleri hareketleri uygulamadan önce öndeki açıklık kontrol ediliyor; ön tarafta yakın bir engel varsa ileri hareket clamp’leniyor, yani güvenlik şartı uygulanıyor. Her bölüm maksimum 400 adım sürebiliyor; çarpışma veya hedefe ulaşma ile erken sonlanıyor.

Ödül tasarımı birkaç bileşenden oluşuyor. Hedefe ilerlemeyi teşvik eden ilerleme ödülü var. Her adım için küçük bir zaman cezası uygulanıyor; dönerken bu ceza daha düşük tutulabiliyor ki manevralar cezalandırılmasın. Baş yönü hedef yönüne yakınsa ekstra bir hizalanma bonusu veriliyor. Ray okumaları belirli bir eşik altında ise güvenlik cezası uygulanıyor; öndeki alan tıkalıysa boş tarafa dönmeyi teşvik eden küçük bir yönsel sezgi ödülü ekleniyor. Ayrıca osilasyon davranışlarını azaltmak için tekrarlı küçük sağ-sol manevralarına ceza getiriliyor. Terminal durumda hedefe ulaşınca +1.0, çarpışmada −1.0 gibi net ödüller var. Bu şekillendirme ajanın kararlı, akıcı ve güvenli hareket etmesini sağlamaya yöneliktir.

Öğrenme tarafında Proximal Policy Optimization (PPO) kullanıldı, Stable-Baselines3 ile. Politikada iki katmanlı bir MLP var, katmanlar 128-128 büyüklüğünde. Öne çıkan hiperparametreler: rollout length 4096, batch size 1024, öğrenme hızı 3e-4, γ=0.99, λ=0.95, clip range 0.2 ve entropy coeff 0.01. Eğitim Monitor ve TensorBoard ile loglanıyor; EvalCallback ile periyodik değerlendirme yapılıyor ve en iyi modeller checkpointleniyor. Deneyi rapor ederken önerilen eğitim süresi harita karmaşıklığına göre 200k–500k timesteps arası tutuldu.

Klasik baz hattımız A* oldu. A*’ı kullanmadan önce engeller grid’e rasterize edilip ajan yarıçapı kadar inflate edilerek güvenlik tamponu oluşturuldu. A* 8-bağlantılı grid üzerinde çalışıyor, yani çapraz hareketler hesaba katılıyor. A* deterministik olduğu ve uygun temsil sağlandığında genelde başarı veriyor; ancak genişletilen düğüm sayısı maliyetin göstergesi oluyor.

Karşılaştırmayı adil yapmak için aynı seed ile hem PPO roll-out’u hem de A* çalıştırıldı, böylece her iki yöntem de birebir aynı haritalarda değerlendirildi. Tüm metrikler harmonize edilmiş bir CSV’ye yazıldı; dosya results/baselines.csv. Görseller visualize_results.py, plot_training_curve.py ve plot_trajectory.py ile oluşturuldu; örnek isimler raporda comparison_hard.png, training_curve.png ve trajectory_hard.png olarak geçiyor.

Deney sonuçlarının özeti şöyle: hard zorluk seviyesinde 100 epizotta, seed=123 için PPO başarı oranı yaklaşık 0.94, ortalama yol uzunluğu ≈ 757.73 px, ortalama adım ≈ 151.5 ve ortalama ödül ≈ 134.2 çıktı. Yaklaşık %6 başarısızlık genelde dar koridorlarda gözlendi. A* ise tüm çalışmalarda başarılı oldu (1.00) fakat ortalama yol uzunluğu biraz daha uzundu, ≈ 775.12 px, ve A*’ın genişlettiği düğüm sayısı ortalama ≈ 582.76 ile daha yüksek arama maliyeti gösterdi. Yani PPO kısa yollar bulma eğiliminde; ancak dar-geçit gibi zor geometrilerde bazı başarısızlıklar görüldü. A* güvenilir sonuç veriyor ama arama maliyeti daha yüksek ve yol biraz daha uzun.

Kod düzeni ve hangi dosyaya baksam diye soracaklar için: ortam dinamikleri, sensör modeli ve ödül hesaplaması navigator/env.py içinde. Eğitim döngüsü, logging ve checkpointing train.py. Değerlendirme scriptleri evaluate_rl.py ile evaluate_planners.py. Görselleştirme scriptleri visualize_results.py, plot_training_curve.py ve plot_trajectory.py. Tüm deney sonuçları results/baselines.csv’te toplanıyor.

Çalışma 2D üstten bakış şeklinde, Pygame ile render ediliyor; Gymnasium arayüzü kullanılıyor ve Stable-Baselines3 PPO ile öğrenme yapıldı. Görselleştirme ve veri işleme için tipik Python kütüphaneleri (numpy, pandas, matplotlib vb.) kullanıldı; TensorBoard eğitim eğrilerini tutmak için kullanıldı.