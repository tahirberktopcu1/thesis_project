# Step 5 – Testing and Comparison

## 5.1 Evaluation Goals
The specification requires quantitative comparison between the reinforcement learning agent and classical planners (A*, optionally RRT). To comply, the evaluation pipeline must produce:
- Success (goal reached vs. collision/time-out),
- Path efficiency (total path length, steps),
- Computational effort (e.g., number of nodes expanded for A*),
- Aggregate statistics for reproducibility (means, medians, CSV export).

## 5.2 Evaluation Workflow
1. **Generate identical random scenarios** for both PPO and A*.  
   Each evaluation episode seeds the Gym environment (`env.reset(seed=...)`). The same seed base is passed to A* and PPO scripts to ensure they experience the same obstacle layouts.
2. **A* baseline (`evaluate_planners.py`).**  
   - Rasterises obstacles into a grid with agent-radius inflation.  
   - Runs grid-based A* with 8-neighbour connectivity.  
   - Records success, path length (in pixels), median/mean statistics, and number of expanded nodes.  
   - Supports `--csv results/baselines.csv --tag astar` to append structured rows.
3. **PPO policy evaluation (`evaluate_rl.py`).**  
   - Loads a trained PPO model (default `models/ppo_linear_navigator.zip`).  
   - Executes deterministic rollouts with the same seed base.  
   - Logs success rate, mean steps, mean path length, and cumulative reward.  
   - Appends the same CSV file via `--csv results/baselines.csv --tag ppo`.
4. **Repeat across difficulty tiers** (`--difficulty warmup|medium|hard`) to align with PPO training regimes.

## 5.3 Sample Commands
```bash
# Classical planner (A*)
python evaluate_planners.py \
  --episodes 100 \
  --resolution 12 \
  --difficulty hard \
  --seed 123 \
  --csv results/baselines.csv \
  --tag astar

# PPO policy
python evaluate_rl.py \
  --model models/ppo_linear_navigator.zip \
  --episodes 100 \
  --seed 123 \
  --difficulty hard \
  --csv results/baselines.csv \
  --tag ppo
```

## 5.4 Metrics Captured
| Metric                | A* (evaluate_planners.py)              | PPO (evaluate_rl.py)                  |
|-----------------------|----------------------------------------|---------------------------------------|
| Success rate          | ✔ (goal reached / total episodes)      | ✔ (goal termination vs. failure)      |
| Mean/median path      | ✔ (pixel distance along discrete path) | ✔ (continuous path accumulated)       |
| Steps                 | ✗                                      | ✔ (episode step count)                |
| Nodes expanded        | ✔                                      | ✗                                     |
| Mean reward           | ✗                                      | ✔                                     |
| CSV export            | ✔ (`--csv`, `--tag`)                   | ✔ (`--csv`, `--tag`)                  |

### Example (100 episodes, difficulty=hard, seed=123)

| Algorithm | Success Rate | Mean Path Length (px) | Median Path Length (px) | Mean Steps | Mean Reward | Mean Nodes Expanded |
|-----------|--------------|-----------------------|-------------------------|------------|-------------|---------------------|
| PPO       | 94 %         | 757.73                | –                       | 151.51     | 134.21      | –                   |
| A*        | 100 %        | 775.12                | 769.71                  | –          | –           | 582.76              |

Generated by:
```bash
python evaluate_rl.py --model models/ppo_linear_navigator.zip \
  --episodes 100 --seed 123 --difficulty hard \
  --csv results/baselines.csv --tag ppo
python evaluate_planners.py --episodes 100 --resolution 12 \
  --difficulty hard --seed 123 \
  --csv results/baselines.csv --tag astar
```

## 5.5 Usage Notes
- CSV file uses columns: `algorithm, difficulty, episodes, success_rate, mean_path_length, median_path_length, mean_steps, mean_reward, mean_nodes_expanded`. Empty fields are left blank (e.g., A* has no `mean_reward`).
- Results can be imported into spreadsheets or plotting libraries to create tables and convergence charts for the thesis.
- Additional seeds repeatability: run the scripts multiple times with different `--seed` values (e.g., 123, 456, 789) and merge CSV files for statistical confidence.
- RRT/RRT* baselines can be added later using the same CSV schema if time permits.

This completes methodology Step 5 by providing a reproducible experimental pipeline to gather comparative statistics for classical planners and the PPO navigation policy.
